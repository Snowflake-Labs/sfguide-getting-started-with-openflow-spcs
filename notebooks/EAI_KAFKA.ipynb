{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "name": "intro"
   },
   "source": [
    "# SPCS Networking Connectivity Test: Apache Kafka\n",
    "\n",
    "**Note: This Notebook should be run in an SPCS Container for testing to be valid**\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook tests SPCS networking connectivity to Kafka clusters in preparation for configuring Snowflake Openflow Kafka connectors:\n",
    "\n",
    "- **[Openflow Connector for Kafka](https://docs.snowflake.com/en/user-guide/data-integration/openflow/connectors/kafka/about)** - Ingests real-time events from Kafka topics into Snowflake tables using Snowpipe Streaming\n",
    "- **[Openflow Connector for Snowflake to Kafka](https://docs.snowflake.com/en/user-guide/data-integration/openflow/connectors/snowflake-to-kafka/about)** - Replicates Snowflake tables to Kafka using CDC for real-time insights distribution\n",
    "\n",
    "Both connectors require External Access Integration (EAI) configuration to enable network connectivity from SPCS to your Kafka brokers. This notebook validates that connectivity before deploying Openflow connectors.\n",
    "\n",
    "## Supported Platforms\n",
    "\n",
    "Works with any Kafka distribution including:\n",
    "- **Apache Kafka** (self-hosted)\n",
    "- **AWS MSK** (Amazon's managed Apache Kafka service)\n",
    "- **Confluent Cloud** and **Confluent Platform**\n",
    "- **Redpanda** and **Redpanda Cloud**\n",
    "- Any other Kafka-compatible platform\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Configure your Kafka bootstrap server URL and authentication details\n",
    "2. **(Optional)** Set up PyPI access if confluent-kafka library needs to be installed\n",
    "3. Install the Confluent Kafka Python client library\n",
    "4. Run the connectivity test to verify network access\n",
    "5. If tests fail, create and attach the Kafka External Access Integration (EAI)\n",
    "6. Restart the notebook session and retest\n",
    "7. Once successful, proceed with Openflow connector configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000001",
   "metadata": {
    "name": "step_1"
   },
   "source": [
    "## Step 1: Configure Kafka Connection Settings\n",
    "\n",
    "Update the configuration below with your actual Kafka cluster details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "language": "python",
    "name": "config"
   },
   "outputs": [],
   "source": [
    "# Kafka Connectivity Test Configuration\n",
    "# Update these values with your actual Kafka cluster details\n",
    "\n",
    "# ============================================================================\n",
    "# KAFKA BOOTSTRAP SERVER CONFIGURATION\n",
    "# ============================================================================\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"your-kafka-broker.example.com:9092\"\n",
    "\n",
    "# ============================================================================\n",
    "# AUTHENTICATION CONFIGURATION\n",
    "# ============================================================================\n",
    "KAFKA_SASL_USERNAME = \"your-username-or-api-key\"\n",
    "KAFKA_SASL_PASSWORD = \"your-password-or-api-secret\"\n",
    "\n",
    "# SASL Mechanism\n",
    "# - Options: PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, GSSAPI (Kerberos)\n",
    "KAFKA_SASL_MECHANISM = \"SCRAM-SHA-512\"\n",
    "\n",
    "# Security Protocol\n",
    "# - Most production clusters: \"SASL_SSL\" (SASL over TLS/SSL)\n",
    "# - Options: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL\n",
    "KAFKA_SECURITY_PROTOCOL = \"SASL_SSL\"\n",
    "\n",
    "# ============================================================================\n",
    "# SNOWFLAKE ROLE CONFIGURATION\n",
    "# ============================================================================\n",
    "# This role will be used to create the EAI and other objects if necessary\n",
    "IMPLEMENTATION_ROLE = \"ACCOUNTADMIN\"\n",
    "OPENFLOW_RUNTIME_ROLE = \"OPENFLOWRUNTIMEROLE\"\n",
    "\n",
    "# ============================================================================\n",
    "# AUTO-EXTRACT CONFIGURATION FOR NETWORK RULES\n",
    "# ============================================================================\n",
    "import re\n",
    "\n",
    "# Extract hostname and port from bootstrap servers\n",
    "bootstrap_parts = KAFKA_BOOTSTRAP_SERVERS.split(',')[0].strip()\n",
    "match = re.match(r'([^:]+):(\\d+)', bootstrap_parts)\n",
    "if match:\n",
    "    KAFKA_HOST = match.group(1)\n",
    "    KAFKA_PORT = match.group(2)\n",
    "else:\n",
    "    KAFKA_HOST = bootstrap_parts\n",
    "    KAFKA_PORT = \"9092\"\n",
    "\n",
    "# Extract domain for wildcard rule\n",
    "host_parts = KAFKA_HOST.split('.')\n",
    "if len(host_parts) >= 2:\n",
    "    KAFKA_DOMAIN = '.'.join(host_parts[-2:])\n",
    "else:\n",
    "    KAFKA_DOMAIN = KAFKA_HOST\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"KAFKA CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Bootstrap Server(s): {KAFKA_BOOTSTRAP_SERVERS}\")\n",
    "print(f\"SASL Mechanism: {KAFKA_SASL_MECHANISM}\")\n",
    "print(f\"Security Protocol: {KAFKA_SECURITY_PROTOCOL}\")\n",
    "print(f\"\\nNetwork Rule Configuration:\")\n",
    "print(f\"  Primary Host: {KAFKA_HOST}\")\n",
    "print(f\"  Primary Port: {KAFKA_PORT}\")\n",
    "print(f\"  Domain: {KAFKA_DOMAIN}\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚úì Configuration loaded. Ready to test connectivity...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000003",
   "metadata": {
    "collapsed": false,
    "name": "step_2a"
   },
   "source": [
    "## Step 2a: PyPI Setup (Optional)\n",
    "\n",
    "Run these cells if you need to install the confluent-kafka library from PyPI. This creates the necessary network rules and External Access Integration for PyPI access.\n",
    "\n",
    "**Skip this section if you already have confluent-kafka installed or have PyPI access configured.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000004",
   "metadata": {
    "language": "sql",
    "name": "pypi_eai",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Create Network Rule and External Access Integration for PyPI\n",
    "-- Run this cell to enable installing Python packages from PyPI\n",
    "\n",
    "USE ROLE {{IMPLEMENTATION_ROLE}};\n",
    "\n",
    "CREATE OR REPLACE NETWORK RULE pypi_network_rule\n",
    "  MODE = EGRESS\n",
    "  TYPE = HOST_PORT\n",
    "  VALUE_LIST = ('pypi.org', 'pypi.python.org', 'pythonhosted.org', 'files.pythonhosted.org');\n",
    "\n",
    "CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION pypi_access_integration\n",
    "  ALLOWED_NETWORK_RULES = (pypi_network_rule)\n",
    "  ENABLED = true\n",
    "  COMMENT = 'External Access Integration for PyPI package installation';\n",
    "\n",
    "-- Grant usage on the integration\n",
    "GRANT USAGE ON INTEGRATION pypi_access_integration TO ROLE {{IMPLEMENTATION_ROLE}};\n",
    "\n",
    "SHOW EXTERNAL ACCESS INTEGRATIONS LIKE 'pypi_access_integration';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000005",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "pypi_eai_notebook",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Apply PyPI integration to this notebook\n",
    "-- Run this after creating the PyPI integration above\n",
    "\n",
    "ALTER NOTEBOOK EAI_KAFKA\n",
    "  SET EXTERNAL_ACCESS_INTEGRATIONS = ('pypi_access_integration');\n",
    "\n",
    "-- Restart your Notebook session after applying an EAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000006",
   "metadata": {
    "collapsed": false,
    "name": "step_2b"
   },
   "source": [
    "## Step 2b: Install Confluent Kafka Client Library\n",
    "\n",
    "Make sure PyPI access is configured first if you get connection errors.\n",
    "You can run this cell twice; the first to install the library, the second to confirm it is imported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000007",
   "metadata": {
    "language": "python",
    "name": "install_kafka"
   },
   "outputs": [],
   "source": [
    "# Install the Confluent Kafka Python client library\n",
    "# Make sure PyPI access is configured first if you get connection errors\n",
    "# You can run this cell twice; the first to install the library, the second to confirm it is imported\n",
    "\n",
    "try:\n",
    "    from confluent_kafka import Producer\n",
    "    print(\"‚úÖ confluent-kafka already available\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing confluent-kafka...\")\n",
    "    %pip install confluent-kafka\n",
    "    print(\"‚úÖ confluent-kafka installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000008",
   "metadata": {
    "collapsed": false,
    "name": "step_3"
   },
   "source": [
    "## Step 3: Connectivity Tests\n",
    "\n",
    "Run these test cells to verify network connectivity and authentication to your Kafka cluster.\n",
    "If any tests fail, use the EAI setup cells in Step 4 to configure network access, then restart and retest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000009",
   "metadata": {
    "language": "python",
    "name": "test_3a"
   },
   "outputs": [],
   "source": [
    "### Test 3a: Socket Connectivity\n",
    "\n",
    "# Test basic network connectivity to the Kafka broker\n",
    "import socket\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 3a: SOCKET CONNECTIVITY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTesting connection to {KAFKA_HOST}:{KAFKA_PORT}...\")\n",
    "\n",
    "try:\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    sock.settimeout(10)\n",
    "    result = sock.connect_ex((KAFKA_HOST, int(KAFKA_PORT)))\n",
    "    sock.close()\n",
    "    \n",
    "    if result == 0:\n",
    "        print(f\"‚úÖ SUCCESS: Socket connection established\")\n",
    "        print(f\"   Network access to Kafka broker is working\")\n",
    "    else:\n",
    "        print(f\"‚ùå FAILED: Socket connection failed (error code: {result})\")\n",
    "        print(f\"   Action: Configure EAI in Step 4 below\")\n",
    "        \n",
    "except socket.gaierror as e:\n",
    "    print(f\"‚ùå FAILED: DNS resolution failed\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(f\"   This typically means the network rule is not configured or EAI is not attached\")\n",
    "    print(f\"   Action: Configure EAI in Step 4 below\")\n",
    "    \n",
    "except socket.timeout:\n",
    "    print(f\"‚ùå FAILED: Connection timeout\")\n",
    "    print(f\"   Action: Verify firewall rules and EAI configuration\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå FAILED: Socket error\")\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000010",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "test_3b"
   },
   "outputs": [],
   "source": [
    "### Test 3b: Kafka Producer & Metadata\n",
    "\n",
    "# Test Kafka client connection and fetch cluster metadata\n",
    "from confluent_kafka import Producer, KafkaException\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST 3b: KAFKA PRODUCER & METADATA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nConnecting to Kafka cluster...\")\n",
    "\n",
    "try:\n",
    "    # Create producer configuration\n",
    "    producer_conf = {\n",
    "        'bootstrap.servers': KAFKA_BOOTSTRAP_SERVERS,\n",
    "        'security.protocol': KAFKA_SECURITY_PROTOCOL,\n",
    "        'sasl.mechanism': KAFKA_SASL_MECHANISM,\n",
    "        'sasl.username': KAFKA_SASL_USERNAME,\n",
    "        'sasl.password': KAFKA_SASL_PASSWORD,\n",
    "    }\n",
    "    \n",
    "    # Create producer instance\n",
    "    producer = Producer(producer_conf)\n",
    "    \n",
    "    # Fetch cluster metadata to verify connection\n",
    "    print(f\"  Fetching cluster metadata (timeout: 10s)...\")\n",
    "    metadata = producer.list_topics(timeout=10)\n",
    "    \n",
    "    if metadata and metadata.brokers:\n",
    "        print(f\"\\n‚úÖ SUCCESS: Connected to Kafka cluster\")\n",
    "        print(f\"   Cluster ID: {metadata.cluster_id if hasattr(metadata, 'cluster_id') else 'N/A'}\")\n",
    "        print(f\"   Number of brokers: {len(metadata.brokers)}\")\n",
    "        print(f\"   Number of topics: {len(metadata.topics)}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå FAILED: No broker information received\")\n",
    "        print(f\"   Action: Verify network connectivity and broker configuration\")\n",
    "        \n",
    "except KafkaException as e:\n",
    "    print(f\"\\n‚ùå FAILED: Kafka error\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(f\"   Action: Verify credentials and SASL configuration\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå FAILED: Unexpected error\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(f\"   Action: Check configuration and network access\")\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000011",
   "metadata": {
    "collapsed": false,
    "name": "step_4"
   },
   "source": [
    "## Step 4: EAI Setup (If connectivity tests failed)\n",
    "\n",
    "If connectivity testing fails, you can use the cells below to prepare and implement an EAI suitable for Kafka access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000012",
   "metadata": {
    "language": "sql",
    "name": "kafka_network_rule",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Create Network Rule for Kafka connectivity\n",
    "-- Run this cell if connectivity tests failed\n",
    "\n",
    "-- Wildcard for all brokers in the same domain\n",
    "-- Examples: *.confluent.cloud, *.amazonaws.com, *.redpanda.com, *.example.com\n",
    "\n",
    "USE ROLE {{IMPLEMENTATION_ROLE}};\n",
    "\n",
    "CREATE OR REPLACE NETWORK RULE kafka_access_rule\n",
    "  MODE = EGRESS\n",
    "  TYPE = HOST_PORT\n",
    "  VALUE_LIST = (\n",
    "    -- Specific Kafka bootstrap server\n",
    "    '{{ KAFKA_HOST }}:{{ KAFKA_PORT }}'\n",
    "  )\n",
    "  COMMENT = 'Network rule for Kafka broker access';\n",
    "\n",
    "SHOW NETWORK RULES LIKE 'kafka_%';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000013",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "kafka_eai",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Create External Access Integration for Kafka\n",
    "-- This uses the network rule created above\n",
    "\n",
    "CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION kafka_eai\n",
    "  ALLOWED_NETWORK_RULES = (kafka_access_rule)\n",
    "  ENABLED = TRUE\n",
    "  COMMENT = 'External Access Integration for Kafka connectivity';\n",
    "\n",
    "-- Grant usage on the integration to your roles\n",
    "GRANT USAGE ON INTEGRATION kafka_eai TO ROLE {{IMPLEMENTATION_ROLE}};\n",
    "GRANT USAGE ON INTEGRATION kafka_eai TO ROLE {{OPENFLOW_RUNTIME_ROLE}};\n",
    "\n",
    "SHOW EXTERNAL ACCESS INTEGRATIONS LIKE 'kafka_eai';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000014",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "kafka_eai_notebook",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Attach the EAI to this notebook\n",
    "ALTER NOTEBOOK EAI_KAFKA\n",
    "  SET EXTERNAL_ACCESS_INTEGRATIONS = ('kafka_eai');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000015",
   "metadata": {
    "collapsed": false,
    "name": "step_5"
   },
   "source": [
    "## Step 5: Restart and Retest\n",
    "\n",
    "After creating and setting the EAI on the Notebook:\n",
    "1. **Restart your Notebook session** (this is required for the EAI to take effect)\n",
    "2. Re-run the configuration cell (Step 1)\n",
    "3. Re-run the connectivity test (Step 3)\n",
    "\n",
    "The tests should now pass if the EAI was configured correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20382f77-131d-40a7-9c2a-d035a2cfc921",
   "metadata": {
    "collapsed": false,
    "name": "step_6"
   },
   "source": [
    "## Step 6: Test Kafka Consumer (Optional)\n",
    "\n",
    "This optional step allows you to test consuming messages from a Kafka topic. This is useful for:\n",
    "- Verifying that you can read messages from existing topics\n",
    "- Testing end-to-end connectivity with consumer operations\n",
    "- Validating that your credentials have read permissions\n",
    "\n",
    "The consumer uses the same configuration defined in Step 1 and will consume a configurable number of messages from a specified topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2268e220-2668-4899-beeb-ada55246e167",
   "metadata": {
    "language": "python",
    "name": "test_consumer"
   },
   "outputs": [],
   "source": [
    "# Test Kafka Consumer - Read messages from a topic\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "from confluent_kafka import Consumer, KafkaException, KafkaError\n",
    "\n",
    "\n",
    "def deserialize_json_record(json_bytes: bytes) -> Dict[str, Any]:\n",
    "    \"\"\"Deserialize JSON bytes back to a record.\"\"\"\n",
    "    return json.loads(json_bytes.decode('utf-8'))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONSUMER CONFIGURATION\n",
    "# ============================================================================\n",
    "KAFKA_TOPIC = \"your-topic-name\" # Update with your actual topic name\n",
    "NUM_MESSAGES = 10 # Number of messages to consume\n",
    "CONSUMER_GROUP_ID = \"eai-kafka-notebook\" # Consumer group ID\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"KAFKA CONSUMER TEST\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Topic: {KAFKA_TOPIC}\")\n",
    "print(f\"Messages to consume: {NUM_MESSAGES}\")\n",
    "print(f\"Consumer Group: {CONSUMER_GROUP_ID}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Create consumer configuration\n",
    "    consumer_conf = {\n",
    "        'bootstrap.servers': KAFKA_BOOTSTRAP_SERVERS,\n",
    "        'security.protocol': KAFKA_SECURITY_PROTOCOL,\n",
    "        'sasl.mechanism': KAFKA_SASL_MECHANISM,\n",
    "        'sasl.username': KAFKA_SASL_USERNAME,\n",
    "        'sasl.password': KAFKA_SASL_PASSWORD,\n",
    "        'group.id': CONSUMER_GROUP_ID,\n",
    "        'client.id': CONSUMER_GROUP_ID,\n",
    "        'auto.offset.reset': 'earliest',  # Start from earliest message if no offset exists\n",
    "        'enable.auto.commit': True,\n",
    "    }\n",
    "    \n",
    "    # Create consumer instance\n",
    "    consumer = Consumer(consumer_conf)\n",
    "\n",
    "    # Subscribe to topic\n",
    "    print(f\"\\nüì° Subscribing to topic '{KAFKA_TOPIC}'...\")\n",
    "    consumer.subscribe([KAFKA_TOPIC])\n",
    "    \n",
    "    # Consume messages in batch\n",
    "    print(f\"üì• Consuming up to {NUM_MESSAGES} messages...\\n\")\n",
    "    \n",
    "    # consume() returns a list of messages (or empty list if none available)\n",
    "    messages = consumer.consume(num_messages=NUM_MESSAGES, timeout=60.0)\n",
    "    \n",
    "    if not messages:\n",
    "        print(\"‚è±Ô∏è  No messages available (timeout reached)\")\n",
    "    else:\n",
    "        # Process each message in the batch\n",
    "        for idx, msg in enumerate(messages, start=1):\n",
    "            if msg.error():\n",
    "                print(f\"‚ùå Consumer error: {msg.error()}\")\n",
    "                continue\n",
    "                \n",
    "            # Display message details\n",
    "            print(f\"Message {idx}:\")\n",
    "            print(f\"  Topic: {msg.topic()}\")\n",
    "            print(f\"  Partition: {msg.partition()}\")\n",
    "            print(f\"  Offset: {msg.offset()}\")\n",
    "            print(f\"  Key: {msg.key().decode('utf-8') if msg.key() else None}\")\n",
    "            print(f\"  Value: {msg.value().decode('utf-8') if msg.value() else None}\")\n",
    "            print(f\"  Timestamp: {msg.timestamp()[1] if msg.timestamp()[0] != -1 else 'N/A'}\")\n",
    "            print(\"-\" * 70)\n",
    "    \n",
    "    # Close consumer\n",
    "    consumer.close()\n",
    "    \n",
    "    print(f\"\\n‚úÖ SUCCESS: Consumed {len(messages)} message(s) from topic '{KAFKA_TOPIC}'\")\n",
    "    \n",
    "except KafkaException as e:\n",
    "    print(f\"\\n‚ùå FAILED: Kafka error\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(f\"   Action: Verify topic exists, credentials have read permissions, and network connectivity\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå FAILED: Unexpected error\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(f\"   Action: Check configuration and network access\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa065185-6208-4cb4-a262-7297ad6cb411",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "lastEditStatus": {
   "authorEmail": "james.kinley@snowflake.com",
   "authorId": "1684103632069",
   "authorName": "JKINLEY",
   "lastEditTime": 1760525595823,
   "notebookId": "f5lbkexkeoeitmavmk52",
   "sessionId": "0ac5e625-ac7c-4938-bf8f-68ffa9803a48"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
