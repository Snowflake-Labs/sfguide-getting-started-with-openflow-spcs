{
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "lastEditStatus": {
   "notebookId": "qwzcftetlqpie6jtetos",
   "authorId": "2252081117450",
   "authorName": "PIERREADM",
   "authorEmail": "pierre.lewandowski@snowflake.com",
   "sessionId": "8ce655b9-fc62-4aab-99f1-d76522f2f4fd",
   "lastEditTime": 1762180842254
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "name": "intro",
    "collapsed": false
   },
   "source": "# SPCS Networking Connectivity Test: Azure Confluent Kafka dedicated clusters\n\n**Note: This Notebook should be run in an SPCS Container for testing to be valid**\n\n## Purpose\n\nThis notebook tests SPCS networking connectivity to Confluent Azure Kafka dedicated clusters in preparation for configuring Snowflake Openflow Kafka connectors:\n\n- **[Openflow Connector for Kafka](https://docs.snowflake.com/en/user-guide/data-integration/openflow/connectors/kafka/about)** - Ingests real-time events from Kafka topics into Snowflake tables using Snowpipe Streaming\n- **[Openflow Connector for Snowflake to Kafka](https://docs.snowflake.com/en/user-guide/data-integration/openflow/connectors/snowflake-to-kafka/about)** - Replicates Snowflake tables to Kafka using CDC for real-time insights distribution\n\nBoth connectors require External Access Integration (EAI) configuration to enable network connectivity from SPCS to your Kafka brokers. This notebook validates that connectivity before deploying Openflow connectors.\n\n## Supported Platforms\n\n- **Confluent Cloud** on Azure for dedicated Clusters (single or multi AZ)\n\n## Steps\n\n1. Configure your Kafka bootstrap servers URLs and authentication details\n2. **(Optional)** Set up PyPI access if confluent-kafka library needs to be installed\n3. Install the Confluent Kafka Python client library\n4. Run the connectivity test to verify network access\n5. If tests fail, create and attach the Kafka External Access Integration (EAI)\n6. Restart the notebook session and retest\n7. Once successful, proceed with Openflow connector configuration\n"
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000001",
   "metadata": {
    "name": "step_1"
   },
   "source": [
    "## Step 1: Configure Kafka Connection Settings\n",
    "\n",
    "Update the configuration below with your actual Kafka cluster details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "language": "python",
    "name": "config"
   },
   "outputs": [],
   "source": "# Kafka Connectivity Test Configuration on Azure Confluent\n# Update these values with your actual Kafka cluster details\n\n# ============================================================================\n# KAFKA BOOTSTRAP SERVER CONFIGURATION\n# ============================================================================\nKAFKA_BOOTSTRAP_SERVERS = [\"<cluster_id>.az1.<id>.<region>.azure.confluent.cloud:9092\",\n                           \"<cluster_id>.az2.<id>.<region>.azure.confluent.cloud:9092\",\n                           \"<cluster_id>.az3.<id>.<region>.azure.confluent.cloud:9092\"]\n\n# ============================================================================\n# AUTHENTICATION CONFIGURATION\n# ============================================================================\nKAFKA_SASL_USERNAME = \"Your API key\"\nKAFKA_SASL_PASSWORD = \"Your API key secret\"\n\n# SASL Mechanism\n# - Options: PLAIN is the only method supported by Confluent \nKAFKA_SASL_MECHANISM = \"PLAIN\"\n\n# Security Protocol\n# - Most production clusters: \"SASL_SSL\" (SASL over TLS/SSL)\n# - Options: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL\nKAFKA_SECURITY_PROTOCOL = \"SASL_SSL\"\n\n# ============================================================================\n# SNOWFLAKE ROLE CONFIGURATION\n# ============================================================================\n# This role will be used to create the EAI and other objects if necessary\nIMPLEMENTATION_ROLE = \"ACCOUNTADMIN\"\nOPENFLOW_RUNTIME_ROLE = \"OPENFLOW_ADMIN\"\n\n# ============================================================================\n# AUTO-EXTRACT CONFIGURATION FOR NETWORK RULES\n# ============================================================================\nimport re\n\n# Extract hostname and port from bootstrap servers\nfirst_server = KAFKA_BOOTSTRAP_SERVERS[0].strip()\nmatch = re.match(r'([^:]+):(\\d+)', first_server)\nif match:\n    KAFKA_HOST = match.group(1)\n    KAFKA_PORT = match.group(2)\nelse:\n    KAFKA_HOST = first_server\n    KAFKA_PORT = \"9092\"\n\nprint(\"=\" * 70)\nprint(\"KAFKA CONFIGURATION SUMMARY\")\nprint(\"=\" * 70)\nprint(f\"Bootstrap Server(s): {KAFKA_BOOTSTRAP_SERVERS}\")\nprint(f\"SASL Mechanism: {KAFKA_SASL_MECHANISM}\")\nprint(f\"Security Protocol: {KAFKA_SECURITY_PROTOCOL}\")\nprint(f\"\\nNetwork Rule Configuration:\")\nprint(f\"  Primary Host: {KAFKA_HOST}\")\nprint(f\"  Primary Port: {KAFKA_PORT}\")\nprint(\"=\" * 70)\nprint(\"\\n‚úì Configuration loaded. Ready to test connectivity...\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000003",
   "metadata": {
    "collapsed": false,
    "name": "step_2a"
   },
   "source": [
    "## Step 2a: PyPI Setup (Optional)\n",
    "\n",
    "Run these cells if you need to install the confluent-kafka library from PyPI. This creates the necessary network rules and External Access Integration for PyPI access.\n",
    "\n",
    "**Skip this section if you already have confluent-kafka installed or have PyPI access configured.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000004",
   "metadata": {
    "language": "sql",
    "name": "pypi_eai",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Create Network Rule and External Access Integration for PyPI\n",
    "-- Run this cell to enable installing Python packages from PyPI\n",
    "\n",
    "USE ROLE {{IMPLEMENTATION_ROLE}};\n",
    "\n",
    "CREATE OR REPLACE NETWORK RULE pypi_network_rule\n",
    "  MODE = EGRESS\n",
    "  TYPE = HOST_PORT\n",
    "  VALUE_LIST = ('pypi.org', 'pypi.python.org', 'pythonhosted.org', 'files.pythonhosted.org');\n",
    "\n",
    "CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION pypi_access_integration\n",
    "  ALLOWED_NETWORK_RULES = (pypi_network_rule)\n",
    "  ENABLED = true\n",
    "  COMMENT = 'External Access Integration for PyPI package installation';\n",
    "\n",
    "-- Grant usage on the integration\n",
    "GRANT USAGE ON INTEGRATION pypi_access_integration TO ROLE {{IMPLEMENTATION_ROLE}};\n",
    "\n",
    "SHOW EXTERNAL ACCESS INTEGRATIONS LIKE 'pypi_access_integration';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000005",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "pypi_eai_notebook",
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": "-- Apply PyPI integration to this notebook\n-- Run this after creating the PyPI integration above\n\nALTER NOTEBOOK EAI_KAFKA\n  SET EXTERNAL_ACCESS_INTEGRATIONS = ('pypi_access_integration', 'CONFLUENT_KAFKA_EAI');\n\n-- Restart your Notebook session after applying an EAI"
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000006",
   "metadata": {
    "collapsed": false,
    "name": "step_2b"
   },
   "source": [
    "## Step 2b: Install Confluent Kafka Client Library\n",
    "\n",
    "Make sure PyPI access is configured first if you get connection errors.\n",
    "You can run this cell twice; the first to install the library, the second to confirm it is imported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000007",
   "metadata": {
    "language": "python",
    "name": "install_kafka"
   },
   "outputs": [],
   "source": [
    "# Install the Confluent Kafka Python client library\n",
    "# Make sure PyPI access is configured first if you get connection errors\n",
    "# You can run this cell twice; the first to install the library, the second to confirm it is imported\n",
    "\n",
    "try:\n",
    "    from confluent_kafka import Producer\n",
    "    print(\"‚úÖ confluent-kafka already available\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing confluent-kafka...\")\n",
    "    %pip install confluent-kafka\n",
    "    print(\"‚úÖ confluent-kafka installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000008",
   "metadata": {
    "collapsed": false,
    "name": "step_3_tests"
   },
   "source": "## Step 3: Connectivity Tests\n\nRun these test cells to verify network connectivity and authentication to your Kafka cluster.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000009",
   "metadata": {
    "language": "python",
    "name": "test_connectivity"
   },
   "outputs": [],
   "source": "### Test 3a: Socket Connectivity\n\n# Test basic network connectivity to the Kafka broker\nimport socket\n\nprint(\"=\" * 60)\nprint(\"TEST 3a: SOCKET CONNECTIVITY\")\n\ndef test_socket_connection(host, port, timeout=10):\n    \"\"\"Try to connect to a host:port and return True if successful.\"\"\"\n    try:\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(timeout)\n        result = sock.connect_ex((host, int(port)))\n        sock.close()\n        return result == 0\n    except Exception:\n        return False\n\n# ==============================================================\n# RUN TESTS\n# ==============================================================\nresults = {}\n\nfor server in KAFKA_BOOTSTRAP_SERVERS:\n    host, port = server.split(\":\")\n    success = test_socket_connection(host, port)\n    results[server] = success\n\n# ==============================================================\n# SUMMARIZE RESULTS\n# ==============================================================\nprint(\"=\" * 60)\nif all(results.values()):\n    print(\"‚úÖ SUCCESS: Socket connection to all AZs established\")\nelse:\n    print(\"‚ùå FAILED: One or more AZ connections could not be established\")\nprint(\"-\" * 60)\n\n# Print per-broker results\nfor server, ok in results.items():\n    icon = \"‚úÖ\" if ok else \"‚ùå\"\n    print(f\"{icon} {server}\")\n\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000010",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "test_list_metadata"
   },
   "outputs": [],
   "source": "### Test 3b: Kafka Producer & Metadata\n\n# Test Kafka client connection and fetch cluster metadata\nfrom confluent_kafka import Producer, KafkaException\nimport random\n\nprint(\"=\" * 60)\nprint(\"TEST 3b: KAFKA PRODUCER & METADATA\")\nprint(\"=\" * 60)\nprint(f\"\\nConnecting to Kafka cluster...\")\n\ntry:\n    # Randomly select one bootstrap server from the list\n    BOOTSTRAP_SERVER = random.choice(KAFKA_BOOTSTRAP_SERVERS)\n    print(f\"  Using bootstrap server: {BOOTSTRAP_SERVER}\")\n\n    # Kafka configuration\n    producer_conf = {\n        'bootstrap.servers': BOOTSTRAP_SERVER,\n        'security.protocol': KAFKA_SECURITY_PROTOCOL,\n        'sasl.mechanism': KAFKA_SASL_MECHANISM,\n        'sasl.username': KAFKA_SASL_USERNAME,\n        'sasl.password': KAFKA_SASL_PASSWORD\n    }\n\n    # Create producer instance\n    producer = Producer(producer_conf)\n\n    # Fetch cluster metadata to verify connection\n    print(f\"  Fetching cluster metadata (timeout: 10s)...\")\n    metadata = producer.list_topics(timeout=10)\n\n    if metadata and metadata.brokers:\n        print(f\"\\n‚úÖ SUCCESS: Connected to Kafka cluster\")\n        print(f\"   Cluster ID: {getattr(metadata, 'cluster_id', 'N/A')}\")\n        print(f\"   Number of brokers: {len(metadata.brokers)}\")\n        print(f\" List of brokers: {metadata.brokers}\")\n        print(f\"   Number of topics: {len(metadata.topics)}\")\n    else:\n        print(f\"\\n‚ùå FAILED: No broker information received\")\n        print(f\"   Action: Verify network connectivity and broker configuration\")\n\nexcept KafkaException as e:\n    print(f\"\\n‚ùå FAILED: Kafka error\")\n    print(f\"   Error: {e}\")\n    print(f\"   Action: Verify credentials and SASL configuration\")\n\nexcept Exception as e:\n    print(f\"\\n‚ùå FAILED: Unexpected error\")\n    print(f\"   Error: {e}\")\n    print(f\"   Action: Check configuration and network access\")\n\nprint(\"=\" * 60)\n\n"
  },
  {
   "cell_type": "code",
   "id": "9934815a-9bc5-4d94-a4d1-0ec63554c6e1",
   "metadata": {
    "language": "python",
    "name": "test_create_topic"
   },
   "outputs": [],
   "source": "# ==============================================================\n# CREATE TOPIC ON CONFLUENT CLOUD\n# ==============================================================\n\nfrom confluent_kafka.admin import AdminClient, NewTopic\nfrom confluent_kafka import KafkaException\nimport random\n\n# Randomly select one bootstrap server from the list\nBOOTSTRAP_SERVER = random.choice(KAFKA_BOOTSTRAP_SERVERS)\nprint(f\"  Using bootstrap server: {BOOTSTRAP_SERVER}\")\n\nadmin_conf = {\n    'bootstrap.servers': BOOTSTRAP_SERVER,\n    'security.protocol': KAFKA_SECURITY_PROTOCOL,\n    'sasl.mechanism': KAFKA_SASL_MECHANISM,\n    'sasl.username': KAFKA_SASL_USERNAME,\n    'sasl.password': KAFKA_SASL_PASSWORD\n}\n\ntopic_name = \"example_new_topic2\"\n\ntry:\n    admin = AdminClient(admin_conf)\n\n    # Check if topic already exists\n    metadata = admin.list_topics(timeout=10)\n    if topic_name in metadata.topics:\n        print(f\"‚úÖ Topic '{topic_name}' already exists.\")\n    else:\n        print(f\"üõ†Ô∏è Creating topic '{topic_name}'...\")\n        new_topic = NewTopic(topic=topic_name, num_partitions=3, replication_factor=3)\n        fs = admin.create_topics([new_topic])\n\n        # Wait for operation to finish\n        for topic, f in fs.items():\n            try:\n                f.result()  # raises exception if creation failed\n                print(f\"‚úÖ Topic '{topic}' created successfully!\")\n            except Exception as e:\n                print(f\"‚ùå Failed to create topic {topic}: {e}\")\n\nexcept KafkaException as e:\n    print(f\"Kafka error: {e}\")\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c3b337ab-4e18-4f73-98ef-bbee29a5e730",
   "metadata": {
    "language": "python",
    "name": "test_read_topics"
   },
   "outputs": [],
   "source": "# ==============================================================\n# READ FROM EXISTING TOPIC\n# ==============================================================\n\nfrom confluent_kafka import Consumer, KafkaException\nimport random\n\ntopic_name = \"topic1\"  # <---- set your topic name\n\n# Randomly select one bootstrap server from the list\nBOOTSTRAP_SERVER = random.choice(KAFKA_BOOTSTRAP_SERVERS)\nprint(f\"  Using bootstrap server: {BOOTSTRAP_SERVER}\")\n\nconsumer_conf = {\n    'bootstrap.servers': BOOTSTRAP_SERVER,\n    'security.protocol': KAFKA_SECURITY_PROTOCOL,\n    'sasl.mechanism': KAFKA_SASL_MECHANISM,\n    'sasl.username': KAFKA_SASL_USERNAME,\n    'sasl.password': KAFKA_SASL_PASSWORD,\n    'group.id': 'dummygroupid',\n    'auto.offset.reset': 'latest'  # start from beginning if no offsets committed\n}\n\nconsumer = Consumer(consumer_conf)\nconsumer.subscribe([topic_name])\n\nprint(f\"üì° Listening for messages on topic '{topic_name}'...\")\n\nmessage_count = 0\nmax_messages = 5\n\ntry:\n    while message_count < max_messages:\n        msg = consumer.poll(timeout=1.0)\n        if msg is None:\n            continue\n        if msg.error():\n            print(f\"‚ö†Ô∏è Consumer error: {msg.error()}\")\n            continue\n\n        print(f\"üß© Received message: {msg.value().decode('utf-8')} (partition {msg.partition()})\")\n        message_count += 1\n\nexcept KeyboardInterrupt:\n    print(\"üõë Stopping consumer...\")\n\nfinally:\n    print(f\"‚úÖ Processed {message_count} messages. Closing consumer.\")\n    consumer.close()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000015",
   "metadata": {
    "collapsed": false,
    "name": "step_4"
   },
   "source": "## Step 4: Restart and Retest\n\nAfter creating and setting the EAI on the Notebook:\n1. **Restart your Notebook session** (this is required for the EAI to take effect)\n2. Re-run the configuration cell (Step 1)\n3. Re-run the connectivity test (Step 3)\n\nThe tests should now pass if the EAI was configured correctly.\n"
  }
 ]
}